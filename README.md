# Otimiza√ß√£o pelo Vetor Gradiente

**Integrantes do Grupo 3**

* Eduardo
* Marcelo
* Wesley

---

## üìò Descri√ß√£o Geral

Este projeto apresenta a implementa√ß√£o pr√°tica do **m√©todo do gradiente** para otimiza√ß√£o de fun√ß√µes multivari√°veis. O trabalho √© dividido em tarefas que envolvem determinar pontos de m√≠nimo e m√°ximo de fun√ß√µes atribu√≠das ao grupo, explorando o comportamento da converg√™ncia com diferentes tamanhos de passo (Œ±) e m√©todos de atualiza√ß√£o.

O notebook principal √© **`Otimiza√ß√£o pelo vetor gradiente.ipynb`**, e nele est√£o as implementa√ß√µes das fun√ß√µes e rotinas solicitadas nas tarefas.

---

## üßÆ Fun√ß√µes atribu√≠das ao Grupo 3

f(x,y) = 3x^2 + x y + y^2 + 2x + y

g(x,y) = sqrt(x^2 + y^2 + 3) + x^2 e^{-y^2} + (x - 2)^2

h(x,y) = 4 e^{-x^2 - y^2} + 3 e^{-x^2 - y^2 + 4x + 6y - 13} - x^2/7 - y^2/12 + 2


---

## üìÇ Estrutura das Tarefas

### üß© **Tarefa 1 (40%) ‚Äî M√≠nimo de f(x, y)**

**Objetivo:** Determinar o ponto de m√≠nimo da fun√ß√£o (f(x, y)) usando o m√©todo do gradiente com passo fixo.

**Etapas:**

1. Plotar o gr√°fico de (f(x, y)) e observar seu ponto de m√≠nimo.
![f.png](f.png)
2. Determinar o vetor gradiente (‚àáf(x, y)).
$$ \nabla f = (\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}) = (6x + y + 2, x + 2y + 1) $$

3. Implementar o m√©todo do gradiente com passo fixo ((Œ± = 0.1)) e estimativa inicial ((x_0, y_0) = (0, 0)), com toler√¢ncia de (10^{-5}).
4. Repetir o processo para (Œ± = 0.15, 0.2, 0.3, 0.5).
5. Analisar o n√∫mero de itera√ß√µes e o comportamento da converg√™ncia.


**Trecho de c√≥digo:**

```python
import numpy as np

def f(x, y):
    return 3*x**2 + x*y + y**2 + 2*x + y

def grad_f(x, y):
    df_dx = 6*x + y + 2
    df_dy = x + 2*y + 1
    return np.array([df_dx, df_dy])

def gradient_descent(alpha, tol=1e-5, max_iter=10000):
    x, y = 0.0, 0.0  # ponto inicial
    for i in range(max_iter):
        grad = grad_f(x, y)
        new_x, new_y = x - alpha * grad[0], y - alpha * grad[1]
        if np.linalg.norm([new_x - x, new_y - y]) < tol:
            break
        x, y = new_x, new_y
    return (x, y), i

for alpha in [0.1, 0.15, 0.2, 0.3, 0.5]:
    ponto, iteracoes = gradient_descent(alpha)
    print(f"Œ±={alpha:.2f} ‚Üí Ponto m√≠nimo: {ponto}, Itera√ß√µes: {iteracoes}")
```

**Sa√≠da esperada (exemplo):**

```
Ponto inicial: (0, 0)
Œ±=0.10 ‚Üí Ponto minimo: (-0.272728, -0.3636309)
N de itera√ß√µes: 56

Œ±=0.15 ‚Üí Ponto minimo: (-0.272728, -0.3636319)
N de itera√ß√µes: 36

Œ±=0.20 ‚Üí Ponto minimo: (-0.272728, -0.3636310)
N de itera√ß√µes: 25

Œ±=0.30 ‚Üí Ponto minimo: (-0.2727288, -0.3636367)
N de itera√ß√µes: 89

Œ±=0.50 ‚Üí Ponto minimo: (nan, -inf)
N de itera√ß√µes: 10000
Overflow!

```

**Discuss√£o:**

* Passos menores resultam em converg√™ncia mais lenta por√©m est√°vel.

| Œ±   | Resultado             | Observa√ß√£o                          |
| --- | --------------------- | ----------------------------------- |
| 0.1 | Convergiu             | Est√°vel                             |
| 0.2 | Convergiu mais r√°pido | Est√°vel                             |
| 0.3 | Oscila levemente      | Quase est√°vel                       |
| 0.5 | **Overflow**          | Passo grande ‚Üí Diverg√™ncia num√©rica |

* Passos grandes podem causar oscila√ß√£o ou diverg√™ncia dependendo da curvatura local.

---

### üß© **Tarefa 2 (20%) ‚Äî M√≠nimos de g(x, y)**

**Objetivo:** Aplicar o c√≥digo anterior para encontrar os dois pontos de m√≠nimo da fun√ß√£o (g(x, y)).

![alt text](g.png)

$$ g(x,y) = \sqrt{x^2 + y^2 + 3} + x^{2}e^{-y^2}+ (x - 2)^2 $$

$$ \frac{\partial g}{\partial x} = \frac{x}{\sqrt{x^2 + y^2 + 3}} + 2e^{-y^2}x + 2(x-2) $$

$$ \frac{\partial g}{\partial y} = \frac{y}{\sqrt{x^2 + y^2 + 3}} - 2e^{-y^2}x^2y $$

$$ \nabla g = (\frac{\partial g}{\partial x},\frac{\partial g}{\partial y}) = (\frac{x}{\sqrt{x^2 + y^2 + 3}} + 2e^{-y^2}x + 2(x-2),\frac{y}{\sqrt{x^2 + y^2 + 3}} - 2e^{-y^2}x^2y) $$

**Trecho de c√≥digo:**

```python
def g(x, y):
    return np.sqrt(x**2 + y**2 + 3) + x**2 * np.exp(-y**2) + (x - 2)**2

def grad_g(x, y):
    df_dx = (x / np.sqrt(x**2 + y**2 + 3)) + 2*x*np.exp(-y**2) + 2*(x - 2)
    df_dy = (y / np.sqrt(x**2 + y**2 + 3)) - 2*y*x**2*np.exp(-y**2)
    return np.array([df_dx, df_dy])

def gradient_descent_g(x0, y0, alpha=0.1, tol=1e-5, max_iter=10000):
    x, y = x0, y0
    for i in range(max_iter):
        grad = grad_g(x, y)
        new_x, new_y = x - alpha * grad[0], y - alpha * grad[1]
        if np.linalg.norm([new_x - x, new_y - y]) < tol:
            break
        x, y = new_x, new_y
    return (x, y), i

# Encontrando os dois m√≠nimos
p1, it1 = gradient_descent_g(0, 0)
p2, it2 = gradient_descent_g(2, 2)
print(f"M√≠nimo 1: {p1} em {it1} itera√ß√µes")
print(f"M√≠nimo 2: {p2} em {it2} itera√ß√µes")
```

**Resultados observados:**

```
- Ponto inicial 1: (0, 1)
 Œ±=0.10 ‚Üí Ponto minimo: (1.61268636, 1.645467316) 
 Itera√ß√µes: 98


Ponto inicial 2: (0, -1)
Œ±=0.10 ‚Üí Ponto minimo: (1.61268636, -1.645467316)
Itera√ß√µes: 98
```

**Discuss√£o:**

* A fun√ß√£o apresenta dois vales distintos. Dependendo do ponto inicial, o algoritmo converge para um deles.

---

### üß© **Tarefa 3 (20%) ‚Äî M√°ximos de h(x, y)**

**Objetivo:** Adaptar o m√©todo do gradiente para encontrar pontos de **m√°ximo** da fun√ß√£o (h(x, y)).

![alt text](h.png)

$$ h(x,y) = 4e^{-x^{2}-y^{2}} + 3e^{-x^{2}-y^{2}+4x+6y-13} - \frac{x^2}{7} - \frac{y^2}{12} + 2 $$

$$ \frac{\partial h}{\partial x} = -8e^{-x^2-y^2}x-\frac{2x}{7}+3e^{-x^2+4x+6y-y^2-13}(-2x + 4) $$

$$ \frac{\partial h}{\partial y} = -8e^{-x^2-y^2}y-\frac{y}{6}+3e^{-x^2+4x+6y-y^2-13}(6 - 2y) $$

$$ \nabla h = (\frac{\partial h}{\partial x},\frac{\partial h}{\partial y}) = (-8e^{-x^2-y^2}x-\frac{2x}{7}+3e^{-x^2+4x+6y-y^2-13},-8e^{-x^2-y^2}y-\frac{y}{6}+3e^{-x^2+4x+6y-y^2-13}) $$


**Trecho de c√≥digo:**

```python
def h(x, y):
    return 4*np.exp(-x**2 - y**2) + 3*np.exp(-x**2 - y**2 + 4*x + 6*y - 13) - (x**2)/7 - (y**2)/12 + 2

def grad_h(x, y):
    df_dx = (-8*x)*np.exp(-x**2 - y**2) + (-6*x + 4)*np.exp(-x**2 - y**2 + 4*x + 6*y - 13) - 2*x/7
    df_dy = (-8*y)*np.exp(-x**2 - y**2) + (-6*y + 6)*np.exp(-x**2 - y**2 + 4*x + 6*y - 13) - 2*y/12
    return np.array([df_dx, df_dy])

def gradient_ascent_h(x0, y0, alpha=0.05, tol=1e-5, max_iter=10000):
    x, y = x0, y0
    for i in range(max_iter):
        grad = grad_h(x, y)
        new_x, new_y = x + alpha * grad[0], y + alpha * grad[1]  # sinal invertido (subida)
        if np.linalg.norm([new_x - x, new_y - y]) < tol:
            break
        x, y = new_x, new_y
    return (x, y), i

# Encontrando os m√°ximos
p1, it1 = gradient_ascent_h(0, 0)
p2, it2 = gradient_ascent_h(2, 2)
print(f"M√°ximo 1: {p1} em {it1} itera√ß√µes")
print(f"M√°ximo 2: {p2} em {it2} itera√ß√µes")
```

**Resultados observados:**

```
Ponto inicial 1: (0, 0)
Œ±=0.10 ‚Üí Ponto maximo: (2.712395288377267e-06, 4.0685929325659015e-06)
N de itera√ß√µes: 1

Ponto inicial 2: (10, 10)
Œ±=0.10 ‚Üí Ponto minimo: (1.907742565615503, 2.917685246264727)
N de itera√ß√µes: 65
```

**Discuss√£o:**

* O m√©todo de subida no gradiente (ascent) √© an√°logo ao de descida, apenas alterando o sinal do passo.

---

### üí° **Tarefa Desafio (10%) ‚Äî Passo Vari√°vel**

**Objetivo:** Implementar o m√©todo do gradiente com **passo vari√°vel**, comparando o desempenho com o m√©todo de passo fixo da Tarefa 1.

**Trecho de c√≥digo:**

```python
def gradient_descent_variable_step(tol=1e-5, max_iter=10000):
    x, y = 0.0, 0.0
    for i in range(max_iter):
        grad = grad_f(x, y)
        alpha = 1 / (i + 1)  # passo vari√°vel decrescente
        new_x, new_y = x - alpha * grad[0], y - alpha * grad[1]
        if np.linalg.norm([new_x - x, new_y - y]) < tol:
            break
        x, y = new_x, new_y
    return (x, y), i

ponto, iteracoes = gradient_descent_variable_step()
print(f"Ponto m√≠nimo com passo vari√°vel: {ponto}, Itera√ß√µes: {iteracoes}")
```

**Resultados observados:**

```
Ponto inicial: (0, 0)
Œ±=0.10 ‚Üí Ponto minimo: (-0.2727285486985472, -0.36363095853530786)
Itera√ß√µes: 56
```


**Comparativo:**

* O passo vari√°vel tende a suavizar a converg√™ncia, evitando oscila√ß√µes, mas pode exigir mais itera√ß√µes.

---

## üìä Conclus√µes

* O m√©todo do gradiente √© sens√≠vel ao tamanho do passo (Œ±).
* Passos grandes podem causar diverg√™ncia, enquanto passos pequenos garantem estabilidade por√©m aumentam o custo computacional.
* O m√©todo de passo vari√°vel mostrou melhor equil√≠brio entre estabilidade e velocidade de converg√™ncia.

---

## üß∞ Requisitos

* Python 3.8+
* Jupyter Notebook
* Numpy

**Para executar:**

```bash
pip install numpy jupyter
jupyter notebook "Otimiza√ß√£o pelo vetor gradiente.ipynb"
```

---

## üìé Cr√©ditos

Trabalho desenvolvido por **Eduardo, Marcelo e Wesley** ‚Äî INSPER, Curso de Ci√™ncia da Computa√ß√£o.
